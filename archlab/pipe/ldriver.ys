#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Matthew
#
# Describe how and why you modified the baseline code.
#
# First, I included the iaddq function whenever possible
# Then, I altered the loop header test to jump to conditionally jump to
# the Loop instead of Done as this branch would be mispredicted less often
# and altered the others with the same ideology in mind
# I then applied the loop unrolling strategy multiple times (6) until I reached a threshold where
# more loop unrolls became detrimental to the efficiency of the program
# dealing with 6 values at a time and just looping again when there were still more than 6 values to copy left
# and dealing with the last extra cases at the bottom in Finish0-4
# I also replaced an unconditional jmp done command with ret
# I tried to figure further means of speeding up the program and noticed that by using r10/r11 consecutively,
# the program was being slowed down a bit, so I interchanged my use of r10 and r11 to get rid of dependencies/bubbles
# i.e. two instructions that used r10 in a row were changed to an instruction
# that used r10 followed by one that used r11 instead
# And lastly, just in general tried to get rid of obvious redundancies or unnecessary instructions



##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion

xorq %rax, %rax
iaddq $-6, %rdx
jl Finish0
andq %rdx, %rdx
jge Loop
ret

Loop:
	mrmovq (%rdi), %r10
	mrmovq 8(%rdi), %r11
	rmmovq %r10, (%rsi)
	andq %r10, %r10
	jle Loop1
	iaddq $1, %rax
Loop1:
	rmmovq %r11, 8(%rsi)
	andq %r11, %r11
	jle Loop2
	iaddq $1, %rax
Loop2:
	mrmovq 16(%rdi), %r10
	mrmovq 24(%rdi), %r11
	rmmovq %r10, 16(%rsi)
	andq %r10, %r10
	jle Loop3
	iaddq $1, %rax
Loop3:
	rmmovq %r11, 24(%rsi)
	andq %r11, %r11
	jle Loop4
	iaddq $1, %rax
Loop4:
	mrmovq 32(%rdi), %r10
	mrmovq 40(%rdi), %r11
	rmmovq %r10, 32(%rsi)
	andq %r10, %r10
	jle Loop5
	iaddq $1, %rax
Loop5:
	rmmovq %r11, 40(%rsi)
	andq %r11, %r11
	jle Npos
	iaddq $1, %rax
Npos:
	iaddq $48, %rdi
	iaddq $48, %rsi
	iaddq $-6, %rdx
	jge Loop
Finish0:
	iaddq $5, %rdx
	jl Done
	mrmovq (%rdi), %r10
	mrmovq 8(%rdi), %r11
	rmmovq %r10, (%rsi)
	andq %r10, %r10
	jle Finish1
	iaddq $1, %rax
Finish1:
	iaddq $-1, %rdx
	jl Done
	rmmovq %r11, 8(%rsi)
	andq %r11, %r11
	jle Finish2
	iaddq $1, %rax
Finish2:
	iaddq $-1, %rdx
	jl Done
	mrmovq 16(%rdi), %r10
	mrmovq 24(%rdi), %r11
	rmmovq %r10, 16(%rsi)
	andq %r10, %r10
	jle Finish3
	iaddq $1, %rax
Finish3:
	iaddq $-1, %rdx
	jl Done
	rmmovq %r11, 24(%rsi)
	andq %r11, %r11
	jle Finish4
	iaddq $1, %rax
Finish4:
	iaddq $-1, %rdx
	jl Done
	mrmovq 32(%rdi), %r10
	rmmovq %r10, 32(%rsi)
	andq %r10, %r10
	jle Done
	iaddq $1, %rax

##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad 2
	.quad -3
	.quad 4
	.quad -5
	.quad -6
	.quad 7
	.quad -8
	.quad 9
	.quad -10
	.quad -11
	.quad 12
	.quad -13
	.quad -14
	.quad -15
	.quad 16
	.quad 17
	.quad -18
	.quad -19
	.quad 20
	.quad 21
	.quad 22
	.quad 23
	.quad -24
	.quad 25
	.quad 26
	.quad 27
	.quad 28
	.quad -29
	.quad 30
	.quad -31
	.quad 32
	.quad 33
	.quad 34
	.quad 35
	.quad -36
	.quad -37
	.quad 38
	.quad -39
	.quad -40
	.quad -41
	.quad 42
	.quad 43
	.quad -44
	.quad -45
	.quad 46
	.quad 47
	.quad 48
	.quad 49
	.quad 50
	.quad -51
	.quad 52
	.quad -53
	.quad 54
	.quad 55
	.quad -56
	.quad -57
	.quad -58
	.quad -59
	.quad -60
	.quad -61
	.quad -62
	.quad -63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
